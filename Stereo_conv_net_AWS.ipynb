{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import dicom, cv2, re, sys\n",
      "import os, fnmatch, shutil, subprocess\n",
      "import numpy as np\n",
      "from PIL import Image\n",
      "import dicom,  cv2, re, sys\n",
      "import os, fnmatch, shutil, subprocess\n",
      "from IPython.utils import io\n",
      "import numpy as np\n",
      "np.random.seed(1234)\n",
      "import matplotlib as plt\n",
      "%matplotlib inline\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore') # we ignore a RuntimeWarning produced from dividing by zero\n",
      "import os, sys, urllib, gzip\n",
      "import cPickle as pickle\n",
      "sys.setrecursionlimit(10000)\n",
      "import glob\n",
      "from IPython.display import clear_output\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "import numpy as np\n",
      "from IPython.display import Image as IPImage\n",
      "\n",
      "from PIL import Image\n",
      "from lasagne.layers import get_output, InputLayer, DenseLayer, Upscale2DLayer, ReshapeLayer\n",
      "from lasagne.init import GlorotUniform\n",
      "from lasagne.nonlinearities import rectify, leaky_rectify, tanh, sigmoid, softmax\n",
      "from lasagne.updates import nesterov_momentum, adam\n",
      "from lasagne.objectives import categorical_crossentropy, binary_crossentropy\n",
      "from nolearn.lasagne import NeuralNet, BatchIterator, PrintLayerInfo\n",
      "from lasagne.layers import Conv2DLayer as Conv2DLayer\n",
      "from lasagne.layers import MaxPool2DLayer as MaxPool2DLayer\n",
      "import theano \n",
      "import theano.tensor as T\n",
      "import lasagne\n",
      "import time\n",
      "try:\n",
      "    from lasagne.layers.dnn import Conv2DDNNLayer as Conv2DLayer\n",
      "    from lasagne.layers.dnn import MaxPool2DDNNLayer as MaxPool2DLayer\n",
      "    print 'Using cuda_convnet (faster)'\n",
      "except ImportError:\n",
      "    from lasagne.layers import Conv2DLayer as Conv2DLayer\n",
      "    from lasagne.layers import MaxPool2DLayer as MaxPool2DLayer\n",
      "    print 'Using lasagne.layers (slower)'\n",
      "    \n",
      "import theano\n",
      "import theano.tensor as T\n",
      "\n",
      "from lasagne.layers import Layer\n",
      "from lasagne import init\n",
      "from lasagne import nonlinearities\n",
      "from scipy.misc import imresize, imread\n",
      "from PIL import ImageOps\n",
      "import scipy as sp\n",
      "import scipy.ndimage.morphology\n",
      "from skimage.morphology import convex_hull_image\n",
      "from skimage.restoration import denoise_tv_chambolle, denoise_bilateral\n",
      "import matplotlib.cm as cm\n",
      "from scipy.optimize import minimize\n",
      "from math import floor"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_StereoImages(dirpath='/home/ubuntu/DepthMap_dataset'):\n",
      "    # load training data\n",
      "    X , X_left,X_right, y =[], [], [], []\n",
      "    for path in glob.glob('%s/Depth_map/DepthMap*' % dirpath):\n",
      "        with open(path, 'rb') as f:\n",
      "            depthM = imread(f)\n",
      "            size_depthM = depthM.shape\n",
      "            depthM = Image.open(f)\n",
      "            depthM = depthM.convert('L')\n",
      "            depthM = np.array(depthM.getdata(),dtype=np.uint8)\n",
      "        y.append(depthM)\n",
      "    y = np.concatenate(y).reshape(-1, 1, size_depthM[0], size_depthM[1]).astype(np.float32)\n",
      "    \n",
      "    for path in glob.glob('%s/StereoImages/Stereoscopic*' % dirpath):\n",
      "        with open(path, 'rb') as f:\n",
      "            StereoIm = imread(f)\n",
      "            size_StereoIm = StereoIm.shape\n",
      "            StereoIm = Image.open(f)\n",
      "            StereoIm = StereoIm.convert('L')\n",
      "            StereoIm = np.array(StereoIm.getdata(),dtype=np.uint8).reshape(1, size_StereoIm[0], size_StereoIm[1])\n",
      "            Im_left = StereoIm[0,...,0:size_StereoIm[1]/2]\n",
      "            Im_right = StereoIm[0,...,size_StereoIm[1]/2:size_StereoIm[1]]\n",
      "#             StereoIm = StereoIm.reshape(-1,1)\n",
      "        X.append(StereoIm)\n",
      "        X_left.append(Im_left)\n",
      "        X_right.append(Im_right)\n",
      "        \n",
      "    X = np.concatenate((X_left,X_right),1).reshape(-1, 2, size_StereoIm[0], size_StereoIm[1]/2).astype(np.float32)\n",
      "    X_left = np.concatenate(X_left).reshape(-1, 1, size_StereoIm[0], size_StereoIm[1]/2).astype(np.float32)\n",
      "    X_right = np.concatenate(X_right).reshape(-1, 1, size_StereoIm[0], size_StereoIm[1]/2).astype(np.float32)\n",
      "    \n",
      "    \n",
      "    \n",
      "    print X_left.shape, X_right.shape, X.shape\n",
      "    \n",
      "    ii = np.random.permutation(len(X))\n",
      "    X_train = X[ii[floor(len(X)*0.1):]]\n",
      "    X_left_train = X_left[ii[floor(len(X_left)*0.1):]]\n",
      "    X_right_train = X_right[ii[floor(len(X_right)*0.1):]]\n",
      "    y_train = y[ii[floor(len(X)*0.1):]]\n",
      "    \n",
      "    X_valid = X[ii[:floor(len(X)*0.1)]]\n",
      "    X_left_valid = X_left[ii[:floor(len(X_left)*0.1)]]\n",
      "    X_right_valid = X_right[ii[:floor(len(X_right)*0.1)]]\n",
      "    y_valid = y[ii[:floor(len(X)*0.1)]]\n",
      "    \n",
      "    # normalize to zero mean and unity variance\n",
      "    offset = np.mean(X_train, 0)\n",
      "    scale = np.std(X_train, 0).clip(min=1)\n",
      "    \n",
      "    offset_left = np.mean(X_left_train, 0)\n",
      "    scale_left = np.std(X_left_train, 0).clip(min=1)\n",
      "    offset_right = np.mean(X_right_train, 0)\n",
      "    scale_right = np.std(X_right_train, 0).clip(min=1)\n",
      "    \n",
      "    X_train = (X_train - offset) / scale\n",
      "    X_valid = (X_valid - offset) / scale\n",
      "    \n",
      "    X_left_train = (X_left_train - offset_left) / scale_left\n",
      "    X_right_train = (X_right_train - offset_right) / scale_right\n",
      "\n",
      "    X_left_valid = (X_left_valid - offset_left) / scale_left\n",
      "    X_right_valid = (X_right_valid - offset_right) / scale_right\n",
      "    return X_train,X_left_train,X_right_train, y_train, X_valid,X_left_valid,X_right_valid, y_valid\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train,X_left_train,X_right_train, y_train, X_valid,X_left_valid,X_right_valid, y_valid = load_StereoImages()\n",
      "print X_train.shape, y_train.shape, X_valid.shape, y_valid.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# y_train[11,0,...].shape\n",
      "# plt.imshow(y_train[11,0,...],cmap = cm.Greys_r)\n",
      "# plt.show()\n",
      "\n",
      "plt.imshow(X_valid[14,0,...],cmap = cm.Greys_r)\n",
      "plt.show()\n",
      "# plt.imshow(X_train[11,1,...],cmap = cm.Greys_r)\n",
      "# plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from batchNormalization import BatchNormLayer\n",
      "\n",
      "def batch_norm(layer, **kwargs):\n",
      "    nonlinearity = getattr(layer, 'nonlinearity', None)\n",
      "    if nonlinearity is not None:\n",
      "        layer.nonlinearity = nonlinearities.identity\n",
      "    if hasattr(layer, 'b') and layer.b is not None:\n",
      "        del layer.params[layer.b]\n",
      "        layer.b = None\n",
      "    layer = BatchNormLayer(layer, **kwargs)\n",
      "    if nonlinearity is not None:\n",
      "        from lasagne.layers import NonlinearityLayer\n",
      "        layer = NonlinearityLayer(layer, nonlinearity)\n",
      "    return layer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_stereo_cnn(input_var=None):\n",
      "    \n",
      "    conv_num_filters1 = 16\n",
      "    conv_num_filters2 = 32\n",
      "    conv_num_filters3 = 32\n",
      "    conv_num_filters4 = 32\n",
      "    filter_size1 = 7\n",
      "    filter_size2 = 5\n",
      "    filter_size3 = 3\n",
      "    filter_size4 = 3\n",
      "    pool_size = 2\n",
      "    scale_factor = 2\n",
      "    pad_in = 'valid'\n",
      "    pad_out = 'full'\n",
      "\n",
      "    # Input layer, as usual:                                                                                                                                                                                \n",
      "    network = InputLayer(shape=(None,2, X_train.shape[2], X_train.shape[3]),input_var=input_var,name=\"input_layer\")                                                                                                                             \n",
      "        \n",
      "    network = batch_norm(Conv2DLayer(\n",
      "            network, num_filters=conv_num_filters1, filter_size=(filter_size1, filter_size1),pad=pad_in,\n",
      "            nonlinearity=lasagne.nonlinearities.rectify,\n",
      "            W=lasagne.init.GlorotUniform(),name=\"conv1\"))\n",
      "    \n",
      "    network = MaxPool2DLayer(network, pool_size=(pool_size, pool_size),name=\"pool1\")\n",
      "    \n",
      "    network = batch_norm(Conv2DLayer(\n",
      "            network, num_filters=conv_num_filters2, filter_size=(filter_size2, filter_size2),pad=pad_in,\n",
      "            nonlinearity=lasagne.nonlinearities.rectify,\n",
      "            W=lasagne.init.GlorotUniform(),name=\"conv2\"))\n",
      "    \n",
      "    network = MaxPool2DLayer(network, pool_size=(pool_size, pool_size),name=\"pool2\")\n",
      "                                                                                                                                     \n",
      "    network = batch_norm(Conv2DLayer(\n",
      "            network, num_filters=conv_num_filters3, filter_size=(filter_size3, filter_size3),pad=pad_in,\n",
      "            nonlinearity=lasagne.nonlinearities.rectify,\n",
      "            W=lasagne.init.GlorotUniform(),name=\"conv3\"))\n",
      "    \n",
      "    network = MaxPool2DLayer(network, pool_size=(pool_size, pool_size),name=\"pool3\")\n",
      "                                                                                                                                     \n",
      "    network = batch_norm(Conv2DLayer(\n",
      "            network, num_filters=conv_num_filters4, filter_size=(filter_size4, filter_size4),pad=pad_in,\n",
      "            nonlinearity=lasagne.nonlinearities.rectify,\n",
      "            W=lasagne.init.GlorotUniform(),name=\"conv4\"))\n",
      "    \n",
      "    network = batch_norm(Conv2DLayer(\n",
      "            network, num_filters=16, filter_size=(filter_size4, filter_size4),pad=pad_out,\n",
      "            nonlinearity=lasagne.nonlinearities.rectify,\n",
      "            W=lasagne.init.GlorotUniform(),name=\"deconv1\"))\n",
      "    \n",
      "    network = Upscale2DLayer(network, scale_factor=(pool_size, pool_size),name=\"upscale1\")\n",
      "    \n",
      "    network = batch_norm(Conv2DLayer(\n",
      "            network, num_filters=8, filter_size=(filter_size3, filter_size3),pad=pad_out,\n",
      "            nonlinearity=lasagne.nonlinearities.rectify,\n",
      "            W=lasagne.init.GlorotUniform(),name=\"deconv2\"))\n",
      "    \n",
      "    network = Upscale2DLayer(network, scale_factor=(pool_size, pool_size),name=\"upscale2\")\n",
      "    \n",
      "    network = batch_norm(Conv2DLayer(\n",
      "            network, num_filters=4, filter_size=(filter_size2, filter_size2),pad=pad_out,\n",
      "            nonlinearity=lasagne.nonlinearities.rectify,\n",
      "            W=lasagne.init.GlorotUniform(),name=\"deconv3\"))\n",
      "    \n",
      "    network = Upscale2DLayer(network, scale_factor=(pool_size, pool_size),name=\"upscale3\")\n",
      "    \n",
      "    network = batch_norm(Conv2DLayer(\n",
      "            network, num_filters=1, filter_size=(filter_size1, filter_size1),pad=pad_out,\n",
      "            nonlinearity=lasagne.nonlinearities.sigmoid,\n",
      "            W=lasagne.init.GlorotUniform(),name=\"deconv4\"))\n",
      "                                 \n",
      "    return network\n",
      "\n",
      "def build_cnn2(input_var_left=None,input_var_right=None):\n",
      "    \n",
      "    conv_num_filters1 = 16\n",
      "    conv_num_filters2 = 32\n",
      "    conv_num_filters3 = 32\n",
      "    conv_num_filters4 = 32\n",
      "    filter_size1 = 7\n",
      "    filter_size2 = 5\n",
      "    filter_size3 = 3\n",
      "    filter_size4 = 3\n",
      "    pool_size = 2\n",
      "    scale_factor = 2\n",
      "    pad_in = 'valid'\n",
      "    pad_out = 'full'\n",
      "\n",
      "    # Input layers, left and right:                                                                                                                                                                                \n",
      "    input_layer_left = InputLayer(shape=(None,1, X_left_train.shape[2], X_left_train.shape[3]),input_var=input_var_left,name=\"input_layer_left\")   \n",
      "    \n",
      "    input_layer_right = InputLayer(shape=(None,1, X_right_train.shape[2], X_right_train.shape[3]),input_var=input_var_right,name=\"input_layer_right\")   \n",
      "        \n",
      "    \n",
      "    # First Conv layer with batch normalization, left and right:  \n",
      "    Conv1_left = batch_norm(Conv2DLayer(\n",
      "            input_layer_left, num_filters=conv_num_filters1, filter_size=(filter_size1, filter_size1),pad=pad_in,\n",
      "            nonlinearity=lasagne.nonlinearities.rectify,\n",
      "            W=lasagne.init.GlorotUniform(),name=\"conv1_left\"))\n",
      "    \n",
      "    Conv1_right = batch_norm(Conv2DLayer(\n",
      "            input_layer_right, num_filters=conv_num_filters1, filter_size=(filter_size1, filter_size1),pad=pad_in,\n",
      "            nonlinearity=lasagne.nonlinearities.rectify,\n",
      "            W=lasagne.init.GlorotUniform(),name=\"conv1_right\"))\n",
      "    \n",
      "    # First max pool layer, left and right:  \n",
      "    \n",
      "    MaxPool1_left = MaxPool2DLayer(Conv1_left, pool_size=(pool_size, pool_size),name=\"pool1_left\")\n",
      "    \n",
      "    MaxPool1_right = MaxPool2DLayer(Conv1_right, pool_size=(pool_size, pool_size),name=\"pool1_right\")\n",
      "    \n",
      "    \n",
      "    # Second Conv layer with batch normalization, left and right: # Second Conv layer with batch normalization, left and right:  \n",
      "    \n",
      "    Conv2_left = batch_norm(Conv2DLayer(\n",
      "            Conv1_left, num_filters=conv_num_filters2, filter_size=(filter_size2, filter_size2),pad=pad_in,\n",
      "            nonlinearity=lasagne.nonlinearities.rectify,\n",
      "            W=lasagne.init.GlorotUniform(),name=\"conv2_left\"))\n",
      "    \n",
      "    Conv2_right = batch_norm(Conv2DLayer(\n",
      "            Conv1_right, num_filters=conv_num_filters2, filter_size=(filter_size2, filter_size2),pad=pad_in,\n",
      "            nonlinearity=lasagne.nonlinearities.rectify,\n",
      "            W=lasagne.init.GlorotUniform(),name=\"conv2_right\"))\n",
      "    \n",
      "    # Second max pool layer, left and right:  \n",
      "    \n",
      "    MaxPool2_left = MaxPool2DLayer(Conv2_left, pool_size=(pool_size, pool_size),name=\"pool2_left\")\n",
      "    \n",
      "    MaxPool2_right = MaxPool2DLayer(Conv2_right, pool_size=(pool_size, pool_size),name=\"pool2_right\")\n",
      "    \n",
      "    \n",
      "    # Third Conv layer with batch normalization, left and right: \n",
      "    \n",
      "    Conv3_left = batch_norm(Conv2DLayer(\n",
      "            MaxPool2_left, num_filters=conv_num_filters3, filter_size=(filter_size3, filter_size3),pad=pad_in,\n",
      "            nonlinearity=lasagne.nonlinearities.rectify,\n",
      "            W=lasagne.init.GlorotUniform(),name=\"conv3_left\"))\n",
      "    \n",
      "    Conv3_right = batch_norm(Conv2DLayer(\n",
      "            MaxPool2_right, num_filters=conv_num_filters3, filter_size=(filter_size3, filter_size3),pad=pad_in,\n",
      "            nonlinearity=lasagne.nonlinearities.rectify,\n",
      "            W=lasagne.init.GlorotUniform(),name=\"conv3_right\"))\n",
      "    \n",
      "    # Third max pool layer, left and right:  \n",
      "    \n",
      "    MaxPool3_left = MaxPool2DLayer(Conv3_left, pool_size=(pool_size, pool_size),name=\"pool3_left\")\n",
      "    \n",
      "    MaxPool3_right = MaxPool2DLayer(Conv3_right, pool_size=(pool_size, pool_size),name=\"pool3_right\")\n",
      "    \n",
      "    \n",
      "    # Fourth and final downward Conv layer with batch normalization, left and right: \n",
      "                                                                                                                                     \n",
      "    Conv4_left = batch_norm(Conv2DLayer(\n",
      "            MaxPool3_left, num_filters=conv_num_filters4, filter_size=(filter_size4, filter_size4),pad=pad_in,\n",
      "            nonlinearity=lasagne.nonlinearities.rectify,\n",
      "            W=lasagne.init.GlorotUniform(),name=\"conv4_left\"))\n",
      "    \n",
      "    Conv4_right = batch_norm(Conv2DLayer(\n",
      "            MaxPool3_right, num_filters=conv_num_filters4, filter_size=(filter_size4, filter_size4),pad=pad_in,\n",
      "            nonlinearity=lasagne.nonlinearities.rectify,\n",
      "            W=lasagne.init.GlorotUniform(),name=\"conv4_right\"))\n",
      "    \n",
      "    # concatenate layer, join left and right images:\n",
      "    \n",
      "    ConcatLayer = ConcatLayer([Conv4_left,Conv4_right])\n",
      "    \n",
      "    network = batch_norm(Conv2DLayer(\n",
      "            ConcatLayer, num_filters=16, filter_size=(filter_size4, filter_size4),pad=pad_out,\n",
      "            nonlinearity=lasagne.nonlinearities.rectify,\n",
      "            W=lasagne.init.GlorotUniform(),name=\"deconv1\"))\n",
      "    \n",
      "    network = Upscale2DLayer(network, scale_factor=(pool_size, pool_size),name=\"upscale1\")\n",
      "    \n",
      "    network = batch_norm(Conv2DLayer(\n",
      "            network, num_filters=8, filter_size=(filter_size3, filter_size3),pad=pad_out,\n",
      "            nonlinearity=lasagne.nonlinearities.rectify,\n",
      "            W=lasagne.init.GlorotUniform(),name=\"deconv2\"))\n",
      "    \n",
      "    network = Upscale2DLayer(network, scale_factor=(pool_size, pool_size),name=\"upscale2\")\n",
      "    \n",
      "    network = batch_norm(Conv2DLayer(\n",
      "            network, num_filters=4, filter_size=(filter_size2, filter_size2),pad=pad_out,\n",
      "            nonlinearity=lasagne.nonlinearities.rectify,\n",
      "            W=lasagne.init.GlorotUniform(),name=\"deconv3\"))\n",
      "    \n",
      "    network = Upscale2DLayer(network, scale_factor=(pool_size, pool_size),name=\"upscale3\")\n",
      "    \n",
      "    network = batch_norm(Conv2DLayer(\n",
      "            network, num_filters=1, filter_size=(filter_size1, filter_size1),pad=pad_out,\n",
      "            nonlinearity=lasagne.nonlinearities.sigmoid,\n",
      "            W=lasagne.init.GlorotUniform(),name=\"deconv4\"))\n",
      "                                 \n",
      "    return network"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
      "    assert len(inputs) == len(targets)\n",
      "    if shuffle:\n",
      "        indices = np.arange(len(inputs))\n",
      "        np.random.shuffle(indices)\n",
      "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
      "        if shuffle:\n",
      "            excerpt = indices[start_idx:start_idx + batchsize]\n",
      "        else:\n",
      "            excerpt = slice(start_idx, start_idx + batchsize)\n",
      "        yield inputs[excerpt], targets[excerpt]\n",
      "\n",
      "def iterator(X, batchsize):\n",
      "    indices = np.arange(len(X))\n",
      "    for i in range(0, len(X) - batchsize + 1, batchsize):\n",
      "        sli = indices[i:i+batchsize]\n",
      "        yield X[sli]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def save_params(model, fn):\n",
      "    with open(fn, 'w') as wr:\n",
      "        pickle.dump(lasagne.layers.get_all_param_values(model), wr)\n",
      "        \n",
      "def dice_loss(predictions, targets):\n",
      "    dice_index = 2*T.sum(predictions*targets)/((T.sum(predictions)+T.sum(targets)))\n",
      "    return -dice_index\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def main(model='cnn', num_epochs=200):\n",
      "    # Load the dataset                                                                                                                                                                                      \n",
      "    print(\"Loading data...\")\n",
      "    X_train,X_left_train,X_right_train, y_train, X_val,X_left_val,X_right_val, y_val = load_StereoImages()\n",
      "    \n",
      "    indices = np.arange(len(X_train))\n",
      "    np.random.shuffle(indices)\n",
      "    \n",
      "    y_train = y_train[...,1:y_train.shape[2]-1,3:y_train.shape[3]-3]\n",
      "    y_val = y_val[...,1:y_val.shape[2]-1,3:y_val.shape[3]-3]\n",
      "    \n",
      "    print 'X_train type and shape:', X_train.dtype, X_train.shape\n",
      "    print 'X_train.min():', X_train.min()\n",
      "    print 'X_train.max():', X_train.max()\n",
      "\n",
      "    print 'X_val type and shape:', X_val.dtype, X_val.shape\n",
      "    print 'X_val.min():', X_val.min()\n",
      "    print 'X_val.max():', X_val.max()\n",
      "    \n",
      "    print 'y_train type and shape:', y_train.dtype, y_train.shape\n",
      "    print 'y_train.min():', y_train.min()\n",
      "    print 'y_train.max():', y_train.max()\n",
      "\n",
      "    print 'y_val type and shape:', y_val.dtype, y_val.shape\n",
      "    print 'y_val.min():', y_val.min()\n",
      "    print 'y_val.max():', y_val.max()\n",
      "\n",
      "    # Prepare Theano variables for inputs and targets                                                                                                                                                       \n",
      "    input_var = T.tensor4('inputs',dtype=theano.config.floatX)\n",
      "    target_var = T.tensor4('targets',dtype=theano.config.floatX)\n",
      "\n",
      "    # Create neural network model (depending on first command line parameter)                                                                                                                               \n",
      "    print(\"Building model and compiling functions...\")\n",
      "\n",
      "    network = build_stereo_cnn(input_var)\n",
      "    laylist = lasagne.layers.get_all_layers(network)\n",
      "    \n",
      "    for l in laylist:\n",
      "        print l.name, lasagne.layers.get_output_shape(l)\n",
      "        \n",
      "    # Create a loss expression for training, i.e., a scalar objective we want\n",
      "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
      "    prediction = lasagne.layers.get_output(network)\n",
      "    dloss = lasagne.objectives.squared_error(prediction, target_var)\n",
      "    dloss = dloss.mean()\n",
      "#     loss = lasagne.objectives.binary_crossentropy(prediction, target_var)\n",
      "#     loss = loss.mean()\n",
      "    \n",
      "#     dloss = lasagne.objectives.squared_error(prediction, target_var)\n",
      "#     dloss = dloss.mean()\n",
      "    # We could add some weight decay as well here, see lasagne.regularization.\n",
      "\n",
      "    # Create update expressions for training, i.e., how to modify the\n",
      "    # parameters at each training step. Here, we'll use Stochastic Gradient\n",
      "    # Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
      "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
      "#     updates = lasagne.updates.adam(\n",
      "#             loss, params, learning_rate=0.01)\n",
      "    updates2 = lasagne.updates.adam(\n",
      "            dloss, params, learning_rate=0.01)\n",
      "\n",
      "    # Create a loss expression for validation/testing. The crucial difference\n",
      "    # here is that we do a deterministic forward pass through the network,\n",
      "    # disabling dropout layers.\n",
      "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
      "#     test_loss = lasagne.objectives.binary_crossentropy(test_prediction,\n",
      "#                                                             target_var)\n",
      "#     test_loss = test_loss.mean()\n",
      "    \n",
      "    test_loss2 = lasagne.objectives.squared_error(test_prediction,target_var)\n",
      "    test_loss2 = test_loss2.mean()\n",
      "\n",
      "    # Compile a function performing a training step on a mini-batch (by giving\n",
      "    # the updates dictionary) and returning the corresponding training loss:\n",
      "#     train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
      "    \n",
      "    train_fn2 = theano.function([input_var, target_var], dloss, updates=updates2)\n",
      "\n",
      "    # Compile a second function computing the validation loss and accuracy:\n",
      "#     val_fn = theano.function([input_var, target_var], [test_loss])\n",
      "    \n",
      "    val_fn2 = theano.function([input_var, target_var], [test_loss2])\n",
      "\n",
      "    # Finally, launch the training loop.\n",
      "    print(\"Starting training...\")\n",
      "    # We iterate over epochs:                                                                                                                                                                               \n",
      "    for epoch in range(num_epochs):\n",
      "        # In each epoch, we do a full pass over the training data:\n",
      "        train_err = 0\n",
      "        train_batches = 0\n",
      "        start_time = time.time()\n",
      "        for batch in iterate_minibatches(X_train, y_train, 16, shuffle=True):\n",
      "            inputs, targets = batch\n",
      "            train_err += train_fn2(inputs, targets)\n",
      "            train_batches += 1\n",
      "#             print train_fn2(inputs, targets)\n",
      "        # And a full pass over the validation data:\n",
      "        val_err = 0\n",
      "        val_batches = 0\n",
      "        for batch in iterate_minibatches(X_val, y_val, 16, shuffle=False):\n",
      "            inputs, targets = batch\n",
      "            err = val_fn2(inputs, targets)\n",
      "            val_err += err[0]\n",
      "            val_batches += 1\n",
      "\n",
      "        # Then we print the results for this epoch:                                                                                                                                                         \n",
      "        print(\"Epoch {} of {} took {:.3f}s\".format(epoch + 1, num_epochs, time.time() - start_time))\n",
      "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
      "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
      "        print(\"  ratio:\\t\\t{:.6f}\".format((train_err / train_batches)/(val_err / val_batches)))\n",
      "#         filename = \"model4_loss:\\t\\t{:.6f}\".format(train_err / train_batches)\n",
      "#         np.savez(filename, *lasagne.layers.get_all_param_values(network)) \n",
      "        np.savez('model_stereo.npz', *lasagne.layers.get_all_param_values(network)) \n",
      "        !git add model_stereo.npz\n",
      "        !git commit -m \"new stereo model\"\n",
      "        !git push -u origin master\n",
      "        \n",
      "    # After training, we compute and print the test error:                                                                                                                                                  \n",
      "#     test_err = 0\n",
      "#     test_acc = 0\n",
      "#     test_batches = 0\n",
      "#     for batch in iterator(X_test,32):\n",
      "#         err = val_fn(batch)\n",
      "#         test_err += err\n",
      "#         test_batches += 1\n",
      "        \n",
      "#     print(\"Final results:\")\n",
      "#     print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
      "    \n",
      "    # Optionally, you could now dump the network weights to a file like this:                                                                                                                               \n",
      "    np.savez('model_stereo.npz', *lasagne.layers.get_all_param_values(network))  \n",
      "    !git add model_stereo.npz\n",
      "    !git commit -m \"new stereo model\"\n",
      "    !git push -u origin master\n",
      "    #                                                                                                                                                                                                       \n",
      "    # And load them again later on like this:                                                                                                                                                               \n",
      "    # with np.load('model.npz') as f:                                                                                                                                                                       \n",
      "    #     param_values = [f['arr_%d' % i] for i in range(len(f.files))]                                                                                                                                     \n",
      "    # lasagne.layers.set_all_param_values(network, param_values)  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "main(model='cnn', num_epochs=45)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}